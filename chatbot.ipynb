{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92c06eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c3405ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9af832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chat.json') as content:\n",
    "    chat = json.load(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33490692",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = []\n",
    "inputs = []\n",
    "responses={}\n",
    "for intent in chat['intents']:\n",
    "    responses[intent['tag']]=intent['responses']\n",
    "    for lines in intent['input']:\n",
    "        inputs.append(lines)\n",
    "        tags.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3d9207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"inputs\":inputs, \"tags\":tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de55cdd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oi</td>\n",
       "      <td>saudacao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oi, fred</td>\n",
       "      <td>saudacao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fred</td>\n",
       "      <td>saudacao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>freudulino</td>\n",
       "      <td>saudacao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ola</td>\n",
       "      <td>saudacao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>o que posso fazer se eu me sentir sozinha?</td>\n",
       "      <td>sozinho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>eu me sinto sozinha</td>\n",
       "      <td>sozinho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>eu me sinto sozinho</td>\n",
       "      <td>sozinho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>eu me sinto abandonado</td>\n",
       "      <td>sozinho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>eu me sinto abandonada</td>\n",
       "      <td>sozinho</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         inputs      tags\n",
       "0                                            oi  saudacao\n",
       "1                                      oi, fred  saudacao\n",
       "2                                          fred  saudacao\n",
       "3                                    freudulino  saudacao\n",
       "4                                           ola  saudacao\n",
       "..                                          ...       ...\n",
       "96   o que posso fazer se eu me sentir sozinha?   sozinho\n",
       "97                          eu me sinto sozinha   sozinho\n",
       "98                          eu me sinto sozinho   sozinho\n",
       "99                       eu me sinto abandonado   sozinho\n",
       "100                      eu me sinto abandonada   sozinho\n",
       "\n",
       "[101 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9b12d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "tokens = list(\"eu me sinto sozinho sozinho\".lower().split())\n",
    "print(len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6e5282b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'eu': 1, 'me': 2, 'sinto': 3, 'sozinho': 4}\n"
     ]
    }
   ],
   "source": [
    "vocab, index = {}, 1  # start indexing from 1\n",
    "vocab['<pad>'] = 0  # add a padding token\n",
    "for token in tokens:\n",
    "  if token not in vocab:\n",
    "    vocab[token] = index\n",
    "    index += 1\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9205c8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>', 1: 'eu', 2: 'me', 3: 'sinto', 4: 'sozinho'}\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7638fbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "example_sequence = [vocab[word] for word in tokens]\n",
    "print(example_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8ab640b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      example_sequence,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      negative_samples=0)\n",
    "print(len(positive_skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "795802e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3): (sozinho, sinto)\n",
      "(2, 1): (me, eu)\n",
      "(4, 4): (sozinho, sozinho)\n",
      "(2, 4): (me, sozinho)\n",
      "(2, 3): (me, sinto)\n"
     ]
    }
   ],
   "source": [
    "for target, context in positive_skip_grams[:5]:\n",
    "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a7cc54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 0 3 1], shape=(4,), dtype=int64)\n",
      "['me', '<pad>', 'sinto', 'eu']\n"
     ]
    }
   ],
   "source": [
    "# Get target and context words for one positive skip-gram.\n",
    "target_word, context_word = positive_skip_grams[0]\n",
    "\n",
    "# Set the number of negative samples per positive context.\n",
    "num_ns = 4\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
    "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
    "    num_sampled=num_ns,  # number of negative context words to sample\n",
    "    unique=True,  # all the negative samples should be unique\n",
    "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
    "    seed=42,  # seed for reproducibility\n",
    "    name=\"negative_sampling\"  # name of this operation\n",
    ")\n",
    "print(negative_sampling_candidates)\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "077bb281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dimension so you can use concatenation (in the next step).\n",
    "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
    "\n",
    "# Concatenate a positive context word with negative sampled words.\n",
    "context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "\n",
    "# Label the first context word as `1` (positive) followed by `num_ns` `0`s (negative).\n",
    "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "# Reshape the target to shape `(1,)` and context and label to `(num_ns+1,)`.\n",
    "target = tf.squeeze(target_word)\n",
    "context = tf.squeeze(context)\n",
    "label = tf.squeeze(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0455609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 4\n",
      "target_word     : sozinho\n",
      "context_indices : [3 2 0 3 1]\n",
      "context_words   : ['sinto', 'me', '<pad>', 'sinto', 'eu']\n",
      "label           : [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"target_index    : {target}\")\n",
    "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
    "print(f\"context_indices : {context}\")\n",
    "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
    "print(f\"label           : {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "792782e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target  : tf.Tensor(4, shape=(), dtype=int32)\n",
      "context : tf.Tensor([3 2 0 3 1], shape=(5,), dtype=int64)\n",
      "label   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(\"target  :\", target)\n",
    "print(\"context :\", context)\n",
    "print(\"label   :\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "048ded8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "        context_class = tf.expand_dims(\n",
    "            tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "        negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "            true_classes=context_class,\n",
    "            num_true=1,\n",
    "            num_sampled=num_ns,\n",
    "            unique=True,\n",
    "            range_max=vocab_size,\n",
    "            seed=42,\n",
    "            name=\"negative_sampling\")\n",
    "\n",
    "        # Build context and label vectors (for one target word)\n",
    "        negative_sampling_candidates = tf.expand_dims(\n",
    "            negative_sampling_candidates, 1)\n",
    "\n",
    "        context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "        label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "        # Append each element from the training example to global lists.\n",
    "        targets.append(target_word)\n",
    "        contexts.append(context)\n",
    "        labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb959f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7681222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file) as f:\n",
    "    lines = f.read().splitlines()\n",
    "for line in lines[:20]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c799edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d9c7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, create a custom standardization function to lowercase the text and\n",
    "# remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 4096\n",
    "sequence_length = 10\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee0bc58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "afd30a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "39e9a838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(tf.data.AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aba8b3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32777\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3c1a8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n",
      "[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n",
      "[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n",
      "[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n",
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[:5]:\n",
    "    print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c14d5d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32777/32777 [00:31<00:00, 1052.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (64882,)\n",
      "contexts.shape: (64882, 5)\n",
      "labels.shape: (64882, 5)\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=42)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)[:,:,0]\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9db53fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aa75d240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d2247e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "        self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=num_ns+1)\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "        # context: (batch, context)\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        # target: (batch,)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        # word_emb: (batch, embed)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        # context_emb: (batch, context, embed)\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "        # dots: (batch, context)\n",
    "        return dots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "121f495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3b5f10a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e8c9c200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 2s 4ms/step - loss: 1.6083 - accuracy: 0.2304\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5888 - accuracy: 0.5488\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5409 - accuracy: 0.5867\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4587 - accuracy: 0.5645\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3615 - accuracy: 0.5738\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2655 - accuracy: 0.6038\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.1758 - accuracy: 0.6392\n",
      "Epoch 8/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0926 - accuracy: 0.6740\n",
      "Epoch 9/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0154 - accuracy: 0.7060\n",
      "Epoch 10/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9436 - accuracy: 0.7357\n",
      "Epoch 11/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.8769 - accuracy: 0.7621\n",
      "Epoch 12/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.8152 - accuracy: 0.7853\n",
      "Epoch 13/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.7582 - accuracy: 0.8050\n",
      "Epoch 14/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.7058 - accuracy: 0.8219\n",
      "Epoch 15/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.6577 - accuracy: 0.8375\n",
      "Epoch 16/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.6138 - accuracy: 0.8511\n",
      "Epoch 17/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.5737 - accuracy: 0.8635\n",
      "Epoch 18/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.5372 - accuracy: 0.8748\n",
      "Epoch 19/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.5039 - accuracy: 0.8849\n",
      "Epoch 20/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4736 - accuracy: 0.8931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f18803d1b70>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bbd65a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorize_layer.get_vocabulary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5410db39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
